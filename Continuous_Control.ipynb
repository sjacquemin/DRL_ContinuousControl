{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> You will need to create the proper python virtual environment.  The environment specifications can be found in the requirements.txt file.\n",
    "<br>\n",
    "Using Anaconda is recommended.  To begin, execute the code below from your anaconda command prompt:<br>\n",
    "<br>\n",
    "$ conda env create -f environment.yaml\n",
    "<br>\n",
    "<br>\n",
    "Then navigate to the directory for this project where you downloaded it on your PC and launch jupyter notebook from there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Decision Point:</b> <br>\n",
    "<br>\n",
    "Before running the below cell, you must make a decision on whether to run the program with one agent, or an alternate version with twenty agents.  For purposes of experimentation (tuning hyperparameters, etc.) during training, I was originally planning to focus on the single agent version, assuming it would be faster to train and iterate with.  However, I later found that the twenty agent version trained just as quickly.  For the final version of the program, I have used the twenty agent version.\n",
    "<br>\n",
    "<br>\n",
    "Whichever version (single or multiple agents) you choose, be sure that the below cell and the cell in section 9 of this program correspond accordingly.  If you are just curious about the implementation of the program and wish to see how the smart agent works, please proceed with the twenty agent version which is currently set up as the default.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# loading environment for one agent\n",
    "\n",
    "# env = UnityEnvironment(file_name='Reacher_Windows_x86_64/Reacher.exe')\n",
    "\n",
    "# loading environment for multiple agents\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64-multi/Reacher.exe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> The demonstration of a random agent can be helpful in seeing the contrast presented by the smart agent.  However, running this step will take a bit of time so if you are only interested in seeing the smart agent and/or the code involved in creating it, it may be better to skip this step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if rewards[0] != 0.0:\n",
    "        print(rewards)\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building our Actor and Critic Networks using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 'Actor' class builds an actor (policy) network that will map states to actions. Note the use of relu activation functions and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 400, fc2_units = 300):    \n",
    "        \n",
    "        # initializing model parameters\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        return F.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 'Critic' class builds a critic (value) network that maps state,action pairs to Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units = 400, fc2_units = 300, fc3_units = 256, fc4_units = 256):    \n",
    "        \n",
    "        # initializing model parameters\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        xs = self.bn1(xs)\n",
    "        x = torch.cat((xs, action), dim = 1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from model architectures:</b> <br>\n",
    "<br>\n",
    "Model architectures were important in the successful training of the agent, but were not the top driving factors in its success.  I tried multiple architectures with both a differing number of layers and differing numbers of hidden nodes per layer.  In the end, a two layer architecture worked well for both the actor and critic models.  The critic model appeared to be amenable to more complex designs (3+ layers), while any additional complexity added to the actor model resulted in disaster.  Batch normalization also appeared to help the agents train, but was less important compared to the tuning of other hyperparameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating a Noise Class to encourage exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise:\n",
    "    \n",
    "    def __init__(self, size, seed, mu = 0., theta = 0.15, sigma = 0.2, dt = 1):\n",
    "        \n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.dt = dt\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        # reset the noise to mean mu\n",
    "        \n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) * self.dt + self.sigma * np.sqrt(self.dt) * np.array([np.random.randn() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from noise:</b> <br>\n",
    "<br>\n",
    "The initial DDPG implementation applied the random.random function instead of the np.random.randn function now used.  This was a problem given that random.random drew from a uniform distribution which significantly skewed the noise in one direction.  The np.random.rand function draws from a gaussian distribution which eliminates this skew.  Another learning was to keep the values of dt in the class to 1 (or exclude alltogether).  In other DDPG github implementations involving other environments, dt was used with a small value (~1e-2) which suppresses the noise.  While that may be helpful in other environments, it was detrimental here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Building a Replay Buffer for our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters specific to prioritized replay\n",
    "\n",
    "PRIO_REPLAY_ALPHA = 0.60\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 100000\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \n",
    "        # initialize parameters\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # add a new experience to memory\n",
    "        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        #print(len(e))\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        # randomly sample a batch of experiences from memory\n",
    "        \n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.memory)\n",
    "    \n",
    "class PrioReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, prob_alpha = PRIO_REPLAY_ALPHA):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.priorities = np.zeros((buffer_size,), dtype = np.float32)\n",
    "        self.pos = 0\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.memory else 1.0\n",
    "        \n",
    "        # adding a new experience to memory\n",
    "        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(e)\n",
    "        else:\n",
    "            self.memory[self.pos] = e\n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.buffer_size\n",
    "    \n",
    "    def sample(self, beta = 0.4):\n",
    "        \n",
    "        if len(self.memory) == self.buffer_size:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = prios ** self.prob_alpha\n",
    "        \n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.memory), self.batch_size, p = probs)\n",
    "        experiences = [self.memory[idx] for idx in indices]\n",
    "        total = len(self.memory)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        # randomly sampling a batch of experiences from memory\n",
    "        \n",
    "        # experiences = random.sample(self.memory, k = self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), indices, np.array(weights, dtype = np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        # return the current size of memory\n",
    "        \n",
    "        return len(self.memory)    \n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        \n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from replay buffer designs:</b> <br>\n",
    "<br>\n",
    "The cell above contains classes for both a random and a prioritized replay buffer design.  For the single agent class in section 8, there are options to use either buffer.  While prioritized replay appeared to be slightly helpful for the single agent, it did not result in a huge improvement, so I left it out of the multiple agent class to favor model parsimony.  The multiple agent class does function differently in that the agents share the same replay buffer versus each having its own buffer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Initializing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(100000)                                                          # replay buffer size\n",
    "BATCH_SIZE = 200                                                                   # minibatch size\n",
    "GAMMA = 1.0                                                                        # discount factor\n",
    "TAU = 0.001                                                                        # for soft update of target parameters\n",
    "LR_ACTOR = 0.0001                                                                  # learning rate of the actor\n",
    "LR_CRITIC = 0.0001                                                                 # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.000001                                                            # L2 weight decay\n",
    "LEARN_EVERY_T = 4                                                                  # num time steps between each net update\n",
    "LR_DECAY = 1.0                                                                     # learning rate decay factor\n",
    "NB_AGENTS = 20                                                                     # number of agents\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from hyperparameters:</b> <br>\n",
    "<br>\n",
    "<b>Batch size -</b> differing batch sizes did not result in significant changes in performance \n",
    "<br>\n",
    "<b>Gamma -</b>  The discount factor for future rewards.  Differing values of gamma between 0.99 and 1.0 did not change performance.  The instructions for the multiple agent scenario stated that discounting rewards were not to be used, so I left the value at 1.0.\n",
    "<br>\n",
    "<b>TAU -</b>  Tau is used for the soft update of the target model parameters from the current model parameters.  Most implementations favor a value of 0.001 which was used in the end.  I ran an experiment by increasing the value of tau by a factor of 10, but this caused a signficant decline in performance.\n",
    "<br>\n",
    "<b>Learning rates -</b>  The learning rate of the actor did not change performance much when varied.  In contrast, the learning rate of the critic proved to be one of the two top drivers for performance.  Successful training did occur at a value of 0.001 (commonly used in DDPG implementations), but it took much longer.  Lowering the critic learning rate to 0.0001 significantly improved training speed.\n",
    "<br>\n",
    "<b>Weight decay -</b>  Used for regularization.  This was the second of the two top drivers of performance.  The agent was very sensitive to any values of weight decay above 0.00001.  Lowering the values by another power of 10 allows the agent to train successfully.\n",
    "<br>\n",
    "<b>LR decay -</b>  To be used to dynamically decay learning rates over time.  This was not used in the final agent but I built in the option early on.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Building the Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> There are two versions of the agent.  'Agent' creates the agent under the single agent scenario while 'Agent_Multi' creates agents under the multiple agents scenario.  The multi agent version needed to be created seperately due to multiple agents sharing a single replay buffer under the multiple agents scenario, which in turn requires the replay buffer to be outside of the 'Agent' class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, LR_ACTOR = LR_ACTOR, LR_CRITIC = LR_CRITIC, LR_DECAY = LR_DECAY,\n",
    "                beta = BETA_START, prio = False, buffer_size = BUFFER_SIZE, nb_agents = NB_AGENTS):\n",
    "        \n",
    "        # initializing parameters\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.agent_score = []\n",
    "        self.lr_actor = LR_ACTOR\n",
    "        self.lr_critic = LR_CRITIC\n",
    "        self.lr_decay = LR_DECAY\n",
    "        self.nb_agents = NB_AGENTS\n",
    "        \n",
    "        # initializing models from step 4\n",
    "        \n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = LR_ACTOR)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = LR_CRITIC, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # initializing noise and replay buffer from steps 5 and 6\n",
    "        \n",
    "        self.noise = Noise(action_size, random_seed)\n",
    "        if prio == False:\n",
    "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        else:\n",
    "            self.memory = PrioReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "\n",
    "        self.beta = beta # specific to prioritized replay\n",
    "        self.prio = prio # specific to prioritized replay\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.critic_losses_v = 100000\n",
    "        self.critic_loss = 100000\n",
    "        self.actor_loss = 100000\n",
    "        \n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        # save experience in replay memory, and use random sample from buffer to learn\n",
    "        if self.nb_agents >1:\n",
    "            iter_obj = zip(states, actions, rewards, next_states, dones)\n",
    "            for state, action, reward, next_state, done in iter_obj:\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "    def step(self, gamma, beta):\n",
    "    \n",
    "        # learn if enough samples are available in memory\n",
    "        \n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            if self.prio == False:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "            elif self.prio == True:\n",
    "                \n",
    "                # the code under this 'else' statement performs the same operations as those under the 'learn' function, but\n",
    "                    # using the priority replay buffer versus the standard replay buffer\n",
    "                \n",
    "                experiences, batch_indices, batch_weights = self.memory.sample(beta)\n",
    "                states, actions, rewards, next_states, dones = experiences\n",
    "                batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "                actions_next = self.actor_target(next_states)\n",
    "                Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "                Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "                Q_expected = self.critic_local(states, actions)\n",
    "                self.critic_losses_v = batch_weights_v * (Q_expected - Q_targets) ** 2\n",
    "                sample_prios_v = self.critic_losses_v + 1e-5 \n",
    "                self.critic_losses_v = self.critic_losses_v.mean()\n",
    "                sample_prios_v = sample_prios_v[0]\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                self.critic_losses_v.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "                # resuming normal operations under the learn function\n",
    "                \n",
    "                ###### update the actor ######\n",
    "        \n",
    "                # compute actor loss\n",
    "\n",
    "                actions_pred = self.actor_local(states)\n",
    "                self.actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "                # minimize the loss\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                ###### option to decay learning rates (not used in final solution) ######\n",
    "\n",
    "                self.lr_actor *= self.lr_decay\n",
    "                self.lr_critic *= self.lr_decay\n",
    "\n",
    "                ###### update target networks ######\n",
    "\n",
    "                self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "                self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "                \n",
    "                ###### update priorities of replay buffer experiences #####\n",
    "                \n",
    "                self.memory.update_priorities(batch_indices, sample_prios_v.data.cpu().numpy())\n",
    "\n",
    "            \n",
    "    def act(self, state, add_noise = True):\n",
    "        \n",
    "        # returns actions for given state as per current policy\n",
    "        \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        ###### update the critic ######\n",
    "        \n",
    "        # get predicted next state actions and Q values from target models\n",
    "        \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # compute Q targets for current states\n",
    "        \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # compute critic loss\n",
    "        \n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        self.critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        ###### update the actor ######\n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        ###### option to decay learning rates (not used in final solution) ######\n",
    "        \n",
    "        self.lr_actor *= self.lr_decay\n",
    "        self.lr_critic *= self.lr_decay\n",
    "        \n",
    "        ###### update target networks ######\n",
    "        \n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \n",
    "        # soft update model parameters from local to target\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.detach_() # added code\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed = 1)\n",
    "    # this creates the replay buffer - in the multiple agent scenario, all agents share this same replay buffer\n",
    "\n",
    "class Agent_Multi():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, LR_ACTOR = LR_ACTOR, LR_CRITIC = LR_CRITIC, LR_DECAY = LR_DECAY,\n",
    "                beta = BETA_START, prio = False, buffer_size = BUFFER_SIZE, nb_agents = NB_AGENTS, memory = memory):\n",
    "        \n",
    "        # initializing parameters\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.agent_score = []\n",
    "        self.lr_actor = LR_ACTOR\n",
    "        self.lr_critic = LR_CRITIC\n",
    "        self.lr_decay = LR_DECAY\n",
    "        self.nb_agents = NB_AGENTS\n",
    "        \n",
    "        # initializing models from step 4\n",
    "        \n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = LR_ACTOR)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = LR_CRITIC, weight_decay = WEIGHT_DECAY)\n",
    "        \n",
    "        # initializing noise from step 5\n",
    "        \n",
    "        self.noise = Noise(action_size, random_seed)\n",
    "        \n",
    "        self.beta = beta # specific to prioritized replay\n",
    "        self.prio = prio # specific to prioritized replay\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.critic_losses_v = 100000\n",
    "        self.critic_loss = 100000\n",
    "        self.actor_loss = 100000\n",
    "        \n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "        \n",
    "        # save experience in replay memory, and use random sample from buffer to learn\n",
    "        \n",
    "        if self.nb_agents >1:\n",
    "            iter_obj = zip(states, actions, rewards, next_states, dones)\n",
    "            for state, action, reward, next_state, done in iter_obj:\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            \n",
    "    def step(self, gamma, beta):\n",
    "    \n",
    "        # learn if enough samples are available in memory\n",
    "        \n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            if self.prio == False:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "            elif self.prio == True:\n",
    "                \n",
    "                # the code under this 'else' statement performs the same operations as those under the 'learn' function, but\n",
    "                    # using the priority replay buffer versus the standard replay buffer\n",
    "                    \n",
    "                experiences, batch_indices, batch_weights = self.memory.sample(beta)\n",
    "\n",
    "                states, actions, rewards, next_states, dones = experiences\n",
    "                batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "                actions_next = self.actor_target(next_states)\n",
    "                Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "                Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "                Q_expected = self.critic_local(states, actions)\n",
    "                self.critic_losses_v = batch_weights_v * (Q_expected - Q_targets) ** 2\n",
    "                sample_prios_v = self.critic_losses_v + 1e-5 \n",
    "                self.critic_losses_v = self.critic_losses_v.mean()\n",
    "                sample_prios_v = sample_prios_v[0]\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                self.critic_losses_v.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "                # resuming normal operations under the learn function\n",
    "                \n",
    "                ###### update the actor ######\n",
    "        \n",
    "                # compute actor loss\n",
    "\n",
    "                actions_pred = self.actor_local(states)\n",
    "                self.actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "                # minimize the loss\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                ###### option to decay learning rates ######\n",
    "\n",
    "                self.lr_actor *= self.lr_decay\n",
    "                self.lr_critic *= self.lr_decay\n",
    "\n",
    "                ###### update target networks ######\n",
    "\n",
    "                self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "                self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "                \n",
    "                ###### update priorities of replay buffer experiences #####\n",
    "                \n",
    "                self.memory.update_priorities(batch_indices, sample_prios_v.data.cpu().numpy())\n",
    "\n",
    "            \n",
    "    def act(self, state, add_noise = True):\n",
    "        \n",
    "        # returns actions for given state as per current policy\n",
    "        \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        ###### update the critic ######\n",
    "        \n",
    "        # get predicted next state actions and Q values from target models\n",
    "        \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # compute Q targets for current states\n",
    "        \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # compute critic loss\n",
    "        \n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        self.critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        ###### update the actor ######\n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # minimize the loss\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        ###### option to decay learning rates (not used in final solution) ######\n",
    "        \n",
    "        self.lr_actor *= self.lr_decay\n",
    "        self.lr_critic *= self.lr_decay\n",
    "        \n",
    "        ###### update target networks ######\n",
    "        \n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \n",
    "        # soft update model parameters from local to target\n",
    "        \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.detach_() # added code\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights about the learning algorithm:</b> <br>\n",
    "<br>\n",
    "Here I will be discussing the Agent_Multi() class. The agent proceeds along a repeating five stage process:\n",
    "<br>\n",
    "<br>\n",
    "1) The environment is reset and the initial states are passed to the agents\n",
    "<br>\n",
    "2) The agents executes their 'act' functions, which take the states and passes them through the actor networks referenced in section 4 of this notebook.  The actor networks pass the states through two hidden layers of 400 and 300 nodes respectively which are followed by relu activation and batch normalization functions (the critic shares the same architecture).  The resulting output are four floating-point values between -1 and 1 which represent the agents' next actions.  Output from the noise function referenced in section 5 are added to these values to encourage exploration.\n",
    "<br>\n",
    "3) The environment takes in the agents' action outpust and returns values corresponding to the next states, rewards and expressions (labeled 'dones') indicating whether or not the current episode is over.  The agents then take in tuples consisting of the initial states, actions of the agents, rewards, next states and dones and feed these experience tuples to their 'add' functions, which in turn adds the experiences to the agents' shared replay buffer.\n",
    "<br>\n",
    "4) Every four time steps, the agents take a sample of their past experience tuples from their buffer and use the values in the tuples to update their actor and critic model parameters.  The agents accomplish this by executing their 'learn' functions, which update the loss values for both actor and critic networks of each agent, which are in turn backpropogated through the network via ADAM optimizer functions which update the weights for both models.\n",
    "<br>\n",
    "5) Finally, the values for the next states are set to be the values for the initial states and the algorithm repeats steps 2 through 5 until a given episode is complete and repeat steps 1 through 5 in subsequent episodes until the environment is considered solved.\n",
    "<br>\n",
    "<br>\n",
    "Detailed explanations for the hyperparameters used by the algorithm are presented in section 7 of the notebook above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training the Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> Training takes a significant amount of time under either single or multiple agents scenarios.  To skip training and see the smart agent in action, skip section 9 and go directly to section 10.\n",
    "<br>\n",
    "<br>\n",
    "<b>Caution:</b> The below cell runs the single agent version of training.  DO NOT RUN if you would like to proceed using the multiple agent scenario.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "num_agents = 1\n",
    "agent = Agent(state_size, action_size, random_seed = 1, prio = True, nb_agents = 1)\n",
    "\n",
    "def ddpg_single(n_episodes = 1000, max_t = 10000, print_every = 100):  \n",
    "    scores_deque = deque(maxlen = print_every)\n",
    "    scores = []\n",
    "    i = 0                                                                  # used for prioritized replay buffer\n",
    "    beta = BETA_START                                                      # used for prioritized replay buffer\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]                  # reset the environment    \n",
    "        state = env_info.vector_observations                               # get the current state (for each agent)\n",
    "        episode_score = 0.                                                 # initialize/reset the episode score  \n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, add_noise = True)                    # agent selects its action\n",
    "            env_info = env.step(action)[brain_name]                        # the action is passed to the environment\n",
    "            next_state = env_info.vector_observations                      # get next state \n",
    "            reward = env_info.rewards                                      # get rewards\n",
    "            done = env_info.local_done                                     # see if episode finished\n",
    "            episode_score += reward[0]                                     # update the score \n",
    "            agent.add(state, action, reward[0], next_state, done)          # add next experience tuple to the buffer\n",
    "            if t % LEARN_EVERY_T == 0:                                     # update model parameter during every 'LEARN_EVERY_T'\n",
    "                agent.step(GAMMA, beta)                                           # timesteps\n",
    "            state = next_state                                             # next state becomes the current state\n",
    "            i += 1\n",
    "            beta = min(1.0, BETA_START + i * (1.0 - BETA_START) / BETA_FRAMES)\n",
    "            if done[0] == True:                                            # exit loop if episode finished\n",
    "                break\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        if agent.prio == True:\n",
    "            print('Episode Score {}: {:.2f} Loss Critic: {:.2E} Loss Actor: {:.2E}'.format(i_episode, episode_score, agent.critic_losses_v, agent.actor_loss))\n",
    "        else:\n",
    "            print('Episode Score {}: {:.2f} Loss Critic: {:.2E} Loss Actor: {:.2E}'.format(i_episode, episode_score, agent.critic_loss, agent.actor_loss))\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 30:                                    # save models and break if environment solved\n",
    "            print('Environment Solved in ', i_episode, ' episodes.')\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg_single()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Caution:</b> The below cell runs the multiple agent version of training.  DO NOT RUN if you would like to proceed using the single agent scenario.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\Anaconda3-1\\envs\\DRL\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\steph\\Anaconda3-1\\envs\\DRL\\lib\\site-packages\\ipykernel_launcher.py:154: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode1: 0.77949998257682\n",
      "Total score (averaged over agents) this episode2: 0.8284999814815849\n",
      "Total score (averaged over agents) this episode3: 1.3309999702498307\n",
      "Total score (averaged over agents) this episode4: 1.4674999671988134\n",
      "Total score (averaged over agents) this episode5: 1.001499977614734\n",
      "Total score (averaged over agents) this episode6: 1.3229999704286466\n",
      "Total score (averaged over agents) this episode7: 1.729999961331479\n",
      "Total score (averaged over agents) this episode8: 1.6674999627284686\n",
      "Total score (averaged over agents) this episode9: 1.7719999603927075\n",
      "Total score (averaged over agents) this episode10: 2.113999952748412\n",
      "Total score (averaged over agents) this episode11: 2.099499953072507\n",
      "Total score (averaged over agents) this episode12: 2.244999949820331\n",
      "Total score (averaged over agents) this episode13: 2.048999954201271\n",
      "Total score (averaged over agents) this episode14: 1.8524999585933943\n",
      "Total score (averaged over agents) this episode15: 1.8679999582469404\n",
      "Total score (averaged over agents) this episode16: 2.014999954961235\n",
      "Total score (averaged over agents) this episode17: 1.8939999576657973\n",
      "Total score (averaged over agents) this episode18: 1.8564999585039854\n",
      "Total score (averaged over agents) this episode19: 1.5809999646618937\n",
      "Total score (averaged over agents) this episode20: 1.4479999676346782\n",
      "Total score (averaged over agents) this episode21: 2.0809999534860157\n",
      "Total score (averaged over agents) this episode22: 1.7094999617896949\n",
      "Total score (averaged over agents) this episode23: 1.8284999591298343\n",
      "Total score (averaged over agents) this episode24: 1.7064999618567482\n",
      "Total score (averaged over agents) this episode25: 2.008999955095345\n",
      "Total score (averaged over agents) this episode26: 2.143499952089036\n",
      "Total score (averaged over agents) this episode27: 1.9779999557882486\n",
      "Total score (averaged over agents) this episode28: 1.8804999579675454\n",
      "Total score (averaged over agents) this episode29: 1.8574999584816343\n",
      "Total score (averaged over agents) this episode30: 2.047499954234803\n",
      "Total score (averaged over agents) this episode31: 2.1214999525807743\n",
      "Total score (averaged over agents) this episode32: 2.2539999496191627\n",
      "Total score (averaged over agents) this episode33: 2.0304999546147835\n",
      "Total score (averaged over agents) this episode34: 2.297999948635689\n",
      "Total score (averaged over agents) this episode35: 2.053499954100693\n",
      "Total score (averaged over agents) this episode36: 2.120499952603126\n",
      "Total score (averaged over agents) this episode37: 1.8869999578222567\n",
      "Total score (averaged over agents) this episode38: 1.8979999575763913\n",
      "Total score (averaged over agents) this episode39: 1.9739999558776566\n",
      "Total score (averaged over agents) this episode40: 2.027499954681833\n",
      "Total score (averaged over agents) this episode41: 1.9964999553747427\n",
      "Total score (averaged over agents) this episode42: 1.9509999563917497\n",
      "Total score (averaged over agents) this episode43: 2.4844999444670806\n",
      "Total score (averaged over agents) this episode44: 2.02999995462596\n",
      "Total score (averaged over agents) this episode45: 2.233999950066201\n",
      "Total score (averaged over agents) this episode46: 2.277999949082722\n",
      "Total score (averaged over agents) this episode47: 2.0519999541342235\n",
      "Total score (averaged over agents) this episode48: 2.764499938208599\n",
      "Total score (averaged over agents) this episode49: 2.7954999375156886\n",
      "Total score (averaged over agents) this episode50: 3.253999927267418\n",
      "Total score (averaged over agents) this episode51: 3.8599999137222736\n",
      "Total score (averaged over agents) this episode52: 2.9929999331012187\n",
      "Total score (averaged over agents) this episode53: 4.127499907743183\n",
      "Total score (averaged over agents) this episode54: 5.283999881893395\n",
      "Total score (averaged over agents) this episode55: 4.787499892991026\n",
      "Total score (averaged over agents) this episode56: 4.849999891594057\n",
      "Total score (averaged over agents) this episode57: 5.187499884050335\n",
      "Total score (averaged over agents) this episode58: 5.096999886073165\n",
      "Total score (averaged over agents) this episode59: 6.281999859586356\n",
      "Total score (averaged over agents) this episode60: 5.80699987020344\n",
      "Total score (averaged over agents) this episode61: 6.7544998490251595\n",
      "Total score (averaged over agents) this episode62: 6.417499856557696\n",
      "Total score (averaged over agents) this episode63: 6.5989998525008575\n",
      "Total score (averaged over agents) this episode64: 6.994499843660742\n",
      "Total score (averaged over agents) this episode65: 8.016499820817266\n",
      "Total score (averaged over agents) this episode66: 8.64849980669096\n",
      "Total score (averaged over agents) this episode67: 8.069999819621437\n",
      "Total score (averaged over agents) this episode68: 7.169499839749185\n",
      "Total score (averaged over agents) this episode69: 8.373499812837695\n",
      "Total score (averaged over agents) this episode70: 7.057499842252582\n",
      "Total score (averaged over agents) this episode71: 9.463499788474294\n",
      "Total score (averaged over agents) this episode72: 8.336999813653524\n",
      "Total score (averaged over agents) this episode73: 9.075499797146744\n",
      "Total score (averaged over agents) this episode74: 9.253499793168173\n",
      "Total score (averaged over agents) this episode75: 9.518499787244952\n",
      "Total score (averaged over agents) this episode76: 9.763499781768774\n",
      "Total score (averaged over agents) this episode77: 9.142499795649199\n",
      "Total score (averaged over agents) this episode78: 9.05249979766085\n",
      "Total score (averaged over agents) this episode79: 10.111499773990358\n",
      "Total score (averaged over agents) this episode80: 10.22599977143109\n",
      "Total score (averaged over agents) this episode81: 9.931499778013677\n",
      "Total score (averaged over agents) this episode82: 10.0064997763373\n",
      "Total score (averaged over agents) this episode83: 10.368999768234813\n",
      "Total score (averaged over agents) this episode84: 10.519999764859673\n",
      "Total score (averaged over agents) this episode85: 10.136499773431561\n",
      "Total score (averaged over agents) this episode86: 10.462999766133725\n",
      "Total score (averaged over agents) this episode87: 11.806999736092992\n",
      "Total score (averaged over agents) this episode88: 11.753999737277633\n",
      "Total score (averaged over agents) this episode89: 11.987499732058492\n",
      "Total score (averaged over agents) this episode90: 13.048999708332124\n",
      "Total score (averaged over agents) this episode91: 10.888999756611899\n",
      "Total score (averaged over agents) this episode92: 11.037499753292641\n",
      "Total score (averaged over agents) this episode93: 12.406499722693118\n",
      "Total score (averaged over agents) this episode94: 12.883999712020156\n",
      "Total score (averaged over agents) this episode95: 12.831999713182435\n",
      "Total score (averaged over agents) this episode96: 14.528999675251544\n",
      "Total score (averaged over agents) this episode97: 13.835999690741321\n",
      "Total score (averaged over agents) this episode98: 15.434999655000865\n",
      "Total score (averaged over agents) this episode99: 15.111999662220455\n",
      "Total score (averaged over agents) this episode100: 15.327999657392507\n",
      "Episode 100\tAverage Score: 5.57\n",
      "Total score (averaged over agents) this episode101: 16.105999640002835\n",
      "Total score (averaged over agents) this episode102: 16.612999628670536\n",
      "Total score (averaged over agents) this episode103: 17.987999597936856\n",
      "Total score (averaged over agents) this episode104: 18.997999575361607\n",
      "Total score (averaged over agents) this episode105: 20.002499552909295\n",
      "Total score (averaged over agents) this episode106: 21.947999509423987\n",
      "Total score (averaged over agents) this episode107: 21.702499514911324\n",
      "Total score (averaged over agents) this episode108: 20.190999548696\n",
      "Total score (averaged over agents) this episode109: 21.75449951374907\n",
      "Total score (averaged over agents) this episode110: 21.810999512486152\n",
      "Total score (averaged over agents) this episode111: 21.863499511312654\n",
      "Total score (averaged over agents) this episode112: 22.007999508082865\n",
      "Total score (averaged over agents) this episode113: 23.22099948097021\n",
      "Total score (averaged over agents) this episode114: 23.583999472856505\n",
      "Total score (averaged over agents) this episode115: 26.313499411847463\n",
      "Total score (averaged over agents) this episode116: 24.83999944478271\n",
      "Total score (averaged over agents) this episode117: 25.99549941895531\n",
      "Total score (averaged over agents) this episode118: 25.68699942585079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode119: 26.442499408964085\n",
      "Total score (averaged over agents) this episode120: 23.81549946768207\n",
      "Total score (averaged over agents) this episode121: 25.359999433159775\n",
      "Total score (averaged over agents) this episode122: 25.67349942615254\n",
      "Total score (averaged over agents) this episode123: 23.541499473806503\n",
      "Total score (averaged over agents) this episode124: 26.907999398559316\n",
      "Total score (averaged over agents) this episode125: 27.169999392703133\n",
      "Total score (averaged over agents) this episode126: 27.226499391440314\n",
      "Total score (averaged over agents) this episode127: 28.694499358627933\n",
      "Total score (averaged over agents) this episode128: 29.6389993375167\n",
      "Total score (averaged over agents) this episode129: 28.329999366775166\n",
      "Total score (averaged over agents) this episode130: 29.738999335281534\n",
      "Total score (averaged over agents) this episode131: 29.829999333247606\n",
      "Total score (averaged over agents) this episode132: 31.349499299284037\n",
      "Total score (averaged over agents) this episode133: 29.207499347161527\n",
      "Total score (averaged over agents) this episode134: 31.31349930008872\n",
      "Total score (averaged over agents) this episode135: 29.488999340869466\n",
      "Total score (averaged over agents) this episode136: 29.48599934093651\n",
      "Total score (averaged over agents) this episode137: 30.988499307353\n",
      "Total score (averaged over agents) this episode138: 29.434499342087683\n",
      "Total score (averaged over agents) this episode139: 29.63849933752795\n",
      "Total score (averaged over agents) this episode140: 30.28149932315572\n",
      "Total score (averaged over agents) this episode141: 31.08149930527433\n",
      "Total score (averaged over agents) this episode142: 31.231499301921584\n",
      "Total score (averaged over agents) this episode143: 30.946499308291866\n",
      "Total score (averaged over agents) this episode144: 31.62849929304793\n",
      "Total score (averaged over agents) this episode145: 32.627999270707406\n",
      "Total score (averaged over agents) this episode146: 31.223999302089233\n",
      "Total score (averaged over agents) this episode147: 30.439999319612983\n",
      "Total score (averaged over agents) this episode148: 32.20099928025159\n",
      "Total score (averaged over agents) this episode149: 32.325999277457576\n",
      "Total score (averaged over agents) this episode150: 31.61399929337207\n",
      "Total score (averaged over agents) this episode151: 31.020499306637817\n",
      "Total score (averaged over agents) this episode152: 31.7029992913828\n",
      "Total score (averaged over agents) this episode153: 31.92199928648766\n",
      "Total score (averaged over agents) this episode154: 31.14149930393323\n",
      "Total score (averaged over agents) this episode155: 33.79849924454468\n",
      "Total score (averaged over agents) this episode156: 33.28349925605585\n",
      "Total score (averaged over agents) this episode157: 32.81849926644941\n",
      "Total score (averaged over agents) this episode158: 32.18249928066515\n",
      "Total score (averaged over agents) this episode159: 32.197999280318676\n",
      "Total score (averaged over agents) this episode160: 33.28249925607812\n",
      "Total score (averaged over agents) this episode161: 33.532999250479165\n",
      "Total score (averaged over agents) this episode162: 32.571999271959086\n",
      "Total score (averaged over agents) this episode163: 32.20799928009511\n",
      "Total score (averaged over agents) this episode164: 32.76349926767873\n",
      "Total score (averaged over agents) this episode165: 32.23499927949165\n",
      "Total score (averaged over agents) this episode166: 33.81399924419821\n",
      "Total score (averaged over agents) this episode167: 34.47899922933442\n",
      "Total score (averaged over agents) this episode168: 33.12749925954269\n",
      "Total score (averaged over agents) this episode169: 33.23649925710633\n",
      "Total score (averaged over agents) this episode170: 33.14449925916271\n",
      "Total score (averaged over agents) this episode171: 32.91249926434835\n",
      "Total score (averaged over agents) this episode172: 31.43599929735063\n",
      "Total score (averaged over agents) this episode173: 31.787499289494\n",
      "Total score (averaged over agents) this episode174: 31.398499298188852\n",
      "Total score (averaged over agents) this episode175: 32.50399927347904\n",
      "Total score (averaged over agents) this episode176: 32.436999274976564\n",
      "Total score (averaged over agents) this episode177: 32.343999277055225\n",
      "Total score (averaged over agents) this episode178: 31.756499290186945\n",
      "Total score (averaged over agents) this episode179: 33.196999257989276\n",
      "Total score (averaged over agents) this episode180: 31.27249930100518\n",
      "Total score (averaged over agents) this episode181: 31.354499299172392\n",
      "Total score (averaged over agents) this episode182: 30.089499327447307\n",
      "Total score (averaged over agents) this episode183: 30.329499322082818\n",
      "Total score (averaged over agents) this episode184: 30.52749931765726\n",
      "Total score (averaged over agents) this episode185: 31.73149929074568\n",
      "Total score (averaged over agents) this episode186: 31.27099930103873\n",
      "Total score (averaged over agents) this episode187: 30.878999309800562\n",
      "Total score (averaged over agents) this episode188: 31.15049930373209\n",
      "Total score (averaged over agents) this episode189: 31.8184992888011\n",
      "Total score (averaged over agents) this episode190: 30.94199930839242\n",
      "Total score (averaged over agents) this episode191: 29.799999333918088\n",
      "Total score (averaged over agents) this episode192: 29.529999339953044\n",
      "Total score (averaged over agents) this episode193: 30.257999323680988\n",
      "Total score (averaged over agents) this episode194: 30.043999328464295\n",
      "Total score (averaged over agents) this episode195: 30.09799932725732\n",
      "Total score (averaged over agents) this episode196: 28.384499365556973\n",
      "Total score (averaged over agents) this episode197: 29.155499348323815\n",
      "Total score (averaged over agents) this episode198: 29.08049935000018\n",
      "Total score (averaged over agents) this episode199: 28.41349936490882\n",
      "Total score (averaged over agents) this episode200: 30.482499318663034\n",
      "Episode 200\tAverage Score: 29.10\n",
      "Total score (averaged over agents) this episode201: 30.946499308291845\n",
      "Total score (averaged over agents) this episode202: 30.271499323379246\n",
      "Total score (averaged over agents) this episode203: 31.14449930386621\n",
      "Total score (averaged over agents) this episode204: 30.956499308068345\n",
      "Total score (averaged over agents) this episode205: 29.99049932966004\n",
      "Total score (averaged over agents) this episode206: 31.56199929453437\n",
      "Total score (averaged over agents) this episode207: 30.38349932087588\n",
      "Total score (averaged over agents) this episode208: 30.57299931664022\n",
      "Environment Solved in  208  episodes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "num_agents = 20\n",
    "agents = Agent_Multi(state_size, action_size, random_seed = 1, prio = False, nb_agents = 20)\n",
    "\n",
    "def ddpg_multiple(n_episodes = 1000, max_t = 1000000, print_every = 100):  \n",
    "    \n",
    "    scores_deque = deque(maxlen = print_every)\n",
    "    scores = []\n",
    "    env_info = env.reset(train_mode=True)[brain_name]                      \n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        env_info = env.reset()[brain_name]                                   # reset the environment   \n",
    "        states = env_info.vector_observations                                # get the current state (for each agent)\n",
    "        episode_score = 0                                                    # initialize/reset the episode score\n",
    "        t_step = 0                                                           # reset timestep\n",
    "        \n",
    "        for t in range(max_t):\n",
    "\n",
    "            actions = agents.act(states)                                     # agent selects its action\n",
    "            env_info = env.step(actions)[brain_name]                         # the action is passed to the environment\n",
    "            next_states = env_info.vector_observations                       # get next states \n",
    "            rewards = env_info.rewards                                       # get rewards\n",
    "            dones = env_info.local_done                                      # see if episode finished\n",
    "            episode_score += np.mean(rewards)                                # update the score \n",
    "            iter_obj = zip(states, actions, rewards, next_states, dones)\n",
    "            for state, action, reward, next_state, done in iter_obj:         # add next experience tuples to the buffer\n",
    "                memory.add(state, action, reward, next_state, done)\n",
    "            t_step += 1\n",
    "            if t_step % LEARN_EVERY_T == 0:                                  # update model parameter during every 'LEARN_EVERY_T'\n",
    "                if len(memory)>BATCH_SIZE:                                        # timesteps\n",
    "                    experiences = memory.sample()\n",
    "                    agents.learn(experiences, GAMMA)\n",
    "            states = next_states                                             # next states becomes the current states\n",
    "            if np.any(dones):                                                # exit loop if episode finished\n",
    "                break       \n",
    "\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        print('Score (averaged over agents) episode {}: {:.2f}'.format(i_episode, episode_score))\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rRolling 100 Episode\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 30:                                      # save models and break if environment solved\n",
    "            print('Environment Solved in ', i_episode, ' episodes.')\n",
    "            torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg_multiple()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Insights from training:</b> <br>\n",
    "<br>\n",
    "As seen in the above line chart, the agents successfully solve the environment after ~150 episodes.  In addition to the detailed learnings from section 7 involving the tuning of the hyperparameters, I also learned that the agents shouldn't necessarily update their model parameters after every time step as this results in the models becoming unstable.  If instead they learn every 4 or more time steps, training takes longer but they also remain stable for longer periods afterward.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  See the trained smart agents in action!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Run the below cell to see the intelligent agents in action.  Each green ball represents a moving target that each double-jointed arm (each arm being an agent) is trying to maintain contact with.\n",
    "<br>\n",
    "<br>\n",
    "If your unity window closed from earlier, delete the comment '#' from the first line and run the entire cell.  If your unity window is still open, leave the first line commented-out and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\Anaconda3-1\\envs\\DRL\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name='Reacher_Windows_x86_64-multi/Reacher.exe')\n",
    "agents = Agent_Multi(state_size, action_size, random_seed = 1, prio = False, nb_agents = 20)\n",
    "agents.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agents.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "env_info = env.reset(train_mode=False)[brain_name]                    \n",
    "states = env_info.vector_observations     \n",
    "while True:\n",
    "    actions = agents.act(states)     \n",
    "    env_info = env.step(actions)[brain_name]         \n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards   \n",
    "    dones = env_info.local_done\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Ideas for improvement:</b> <br>\n",
    "<br>\n",
    "This solution utilizes a DDPG framework.  PPO or other actor-critic frameworks could be applied and could potentially result in an improved solution.  Another idea is to implement learning rate decay, starting training with large learning rates which gradually decline as the agents train and eventually stablize with very low learning rate values.  Prioritized replay could also be applied to the critic network of the multi-agent solution, although I didn't see a huge increase in performance from this in the single agent scenario.  Double Q networks or other Q-network improvements could also be applied to the critic networks which could potentially improve performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
